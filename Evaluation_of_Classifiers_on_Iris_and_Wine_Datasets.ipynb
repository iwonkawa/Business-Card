{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNoODBMXx9PgP5giqsgNx2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iwonkawa/Business-Card/blob/master/Evaluation_of_Classifiers_on_Iris_and_Wine_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h51kor5ugGxE",
        "outputId": "d1a1ba1d-7a73-46ad-ab7e-3e988f5534fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Decision Tree on Iris dataset\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy: 1.0000\n",
            "Classification Report on Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "------------\n",
            "Evaluating Support Vector Machine on Iris dataset\n",
            "Training Accuracy: 0.9619\n",
            "Testing Accuracy: 1.0000\n",
            "Classification Report on Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "------------\n",
            "Evaluating K-Nearest Neighbors on Iris dataset\n",
            "Training Accuracy: 0.9524\n",
            "Testing Accuracy: 1.0000\n",
            "Classification Report on Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      1.00      1.00        13\n",
            "           2       1.00      1.00      1.00        13\n",
            "\n",
            "    accuracy                           1.00        45\n",
            "   macro avg       1.00      1.00      1.00        45\n",
            "weighted avg       1.00      1.00      1.00        45\n",
            "\n",
            "------------\n",
            "Evaluating Decision Tree on Wine dataset\n",
            "Training Accuracy: 1.0000\n",
            "Testing Accuracy: 0.9630\n",
            "Classification Report on Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97        19\n",
            "           1       0.91      1.00      0.95        21\n",
            "           2       1.00      0.93      0.96        14\n",
            "\n",
            "    accuracy                           0.96        54\n",
            "   macro avg       0.97      0.96      0.96        54\n",
            "weighted avg       0.97      0.96      0.96        54\n",
            "\n",
            "------------\n",
            "Evaluating Support Vector Machine on Wine dataset\n",
            "Training Accuracy: 0.6694\n",
            "Testing Accuracy: 0.7593\n",
            "Classification Report on Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       0.63      0.90      0.75        21\n",
            "           2       0.60      0.21      0.32        14\n",
            "\n",
            "    accuracy                           0.76        54\n",
            "   macro avg       0.74      0.71      0.69        54\n",
            "weighted avg       0.75      0.76      0.72        54\n",
            "\n",
            "------------\n",
            "Evaluating K-Nearest Neighbors on Wine dataset\n",
            "Training Accuracy: 0.7742\n",
            "Testing Accuracy: 0.7407\n",
            "Classification Report on Test Data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89        19\n",
            "           1       0.75      0.71      0.73        21\n",
            "           2       0.53      0.57      0.55        14\n",
            "\n",
            "    accuracy                           0.74        54\n",
            "   macro avg       0.73      0.73      0.73        54\n",
            "weighted avg       0.74      0.74      0.74        54\n",
            "\n",
            "------------\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.datasets import load_iris, load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load datasets\n",
        "iris = load_iris()\n",
        "wine = load_wine()\n",
        "\n",
        "# Split data into training and testing sets (70% training, 30% testing)\n",
        "x_train_iris, x_test_iris, y_train_iris, y_test_iris = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "x_train_wine, x_test_wine, y_train_wine, y_test_wine = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create classifiers\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "svc_clf = SVC()\n",
        "knn_clf = KNeighborsClassifier()\n",
        "\n",
        "# Put all classifiers in a list\n",
        "classifiers = [dt_clf, svc_clf, knn_clf]\n",
        "classifier_names = [\"Decision Tree\", \"Support Vector Machine\", \"K-Nearest Neighbors\"]\n",
        "\n",
        "# Function to train and evaluate classifiers\n",
        "def evaluate_classifiers(x_train, x_test, y_train, y_test, dataset_name):\n",
        "    for clf, name in zip(classifiers, classifier_names):\n",
        "        print(f\"Evaluating {name} on {dataset_name} dataset\")\n",
        "        clf.fit(x_train, y_train)\n",
        "        y_pred_train = clf.predict(x_train)\n",
        "        y_pred_test = clf.predict(x_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "        train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "        print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n",
        "        print(\"Classification Report on Test Data:\")\n",
        "        print(classification_report(y_test, y_pred_test))\n",
        "        print(\"------------\")\n",
        "\n",
        "# Evaluate on Iris dataset\n",
        "evaluate_classifiers(x_train_iris, x_test_iris, y_train_iris, y_test_iris, \"Iris\")\n",
        "\n",
        "# Evaluate on Wine dataset\n",
        "evaluate_classifiers(x_train_wine, x_test_wine, y_train_wine, y_test_wine, \"Wine\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report:**\n",
        "\n",
        "**Datasets Used: **\n",
        "\n",
        "**1.Iris Dataset:** This dataset contains 150 samples of iris flowers, classified into three species: Setosa, Versicolour, and Virginica. The features include sepal length, sepal width, petal length, and petal width.\n",
        "\n",
        "**2.Wine Dataset**:This dataset contains 178 samples of wine, classified into three different classes. The features include chemical properties such as alcohol content, malic acid, ash, alkalinity of ash, and more.\n",
        "\n",
        "**Classifiers Used:**\n",
        "\n",
        "1.Decision Tree Classifier\n",
        "\n",
        "2.Support Vector Machine (SVM)\n",
        "\n",
        "3.K-Nearest Neighbors (KNN)\n",
        "\n",
        "**Data Splitting:**\n",
        "\n",
        "Both datasets were split into training and testing sets with a ratio of 70% training and 30% testing using the train_test_split function. A fixed random seed (42) was used to ensure reproducibility.\n",
        "\n",
        "**Evaluation and Results:**\n",
        "\n",
        "**1. Iris Dataset:**\n",
        "\n",
        "**Decision Tree Classifier:**\n",
        "\n",
        "*   Training Accuracy: 1.0000\n",
        "*   Testing Accuracy: 1.0000\n",
        "*   Observation: The Decision Tree performs well on both training and test sets, with a slight drop in accuracy on the test set, indicating minimal overfitting.\n",
        "\n",
        "\n",
        "\n",
        "**Support Vector Machine:**\n",
        "\n",
        "\n",
        "*   Training Accuracy: 0.9619\n",
        "*   Testing Accuracy: 1.0000\n",
        "*   Observation: The SVM model has slightly lower accuracy on the training set but achieves perfect accuracy on the test set. This indicates that the SVM generalizes well without overfitting, despite not perfectly fitting the training data.\n",
        "\n",
        "\n",
        "**K-Nearest Neighbors:**\n",
        "*   Training Accuracy: 0.9524\n",
        "*   Testing Accuracy: 1.0000\n",
        "*   Observation: KNN also achieves perfect accuracy on the test set, while the training accuracy is slightly lower. This suggests that KNN is a good fit for the Iris dataset, balancing between fitting the training data and generalizing to unseen data.\n",
        "\n",
        "**2.Wine Dataset:**\n",
        "\n",
        "**Decision Tree Classifier:**\n",
        "\n",
        "*   Training Accuracy: 1.0000\n",
        "*   Testing Accuracy: 0.9630\n",
        "*   Observation: The Decision Tree classifier perfectly fits the training data but shows a slight decrease in accuracy on the test set. This suggests that the model is overfitting to some extent, as it captures the training data very well but loses some generalization ability.\n",
        "\n",
        "\n",
        "**Support Vector Machine:**\n",
        "*   Training Accuracy: 0.6694\n",
        "*   Testing Accuracy: 0.7593\n",
        "*   Observation: The SVM model shows moderate performance, with significant underfitting on the training data. This underfitting indicates that the model may not be complex enough to fully capture the patterns in the Wine dataset, resulting in lower accuracy.\n",
        "\n",
        "\n",
        "**K-Nearest Neighbors:**\n",
        "*   Training Accuracy: 0.7742\n",
        "*   Testing Accuracy: 0.7407\n",
        "*   Observation: KNN performs slightly better than SVM on the training data but shows similar accuracy on the test data. The gap between training and test accuracy suggests some degree of underfitting, meaning the model may not be fully capturing the complexity of the Wine dataset.\n",
        "\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "**1.   Iris Dataset:**\n",
        "\n",
        "All three classifiers perform exceptionally well, with perfect accuracy on the test set. This suggests that the Iris dataset is relatively simple, and all models are able to capture the relationships between features and classes effectively. However, the perfect accuracy of the Decision Tree on both training and test sets could indicate overfitting, although it still generalizes well.\n",
        "\n",
        "**2. Wine Dataset:**\n",
        "\n",
        "The Decision Tree classifier shows signs of overfitting, as evidenced by the perfect training accuracy but slightly lower test accuracy. Despite this, it still performs better than the other models.\n",
        "SVM and KNN both show signs of underfitting, particularly on the Wine dataset, indicating that these models might require more tuning or a different approach to handle the complexity of the data.\n",
        "\n",
        "**General Insight:**\n",
        "\n",
        "The results highlight the importance of choosing the right model for a given dataset. While simpler datasets like Iris can be modeled effectively by various classifiers, more complex datasets like Wine may require careful tuning and model selection to avoid overfitting or underfitting. Regularization, pruning, and cross-validation are potential strategies to improve model performance, particularly in cases where overfitting or underfitting is observed."
      ],
      "metadata": {
        "id": "cR8dlgE68i8H"
      }
    }
  ]
}